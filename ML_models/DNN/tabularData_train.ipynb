{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, concatenate, Flatten\n",
    "from keras.layers.core import Dense, Reshape, Lambda\n",
    "from keras.utils import to_categorical\n",
    "from keras import regularizers\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow import set_random_seed\n",
    "import pickle\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "BASE_DIR = \"../..\"\n",
    "DATA_DIR = \"data\"\n",
    "DATA_FILE = \"balanced_data.csv\"\n",
    "WORD_EMBEDDING_DIM = 100\n",
    "GLOVE_DIR = \"glove.6B\"\n",
    "GLOVE_FILE = \"glove.6B.\" +  str(WORD_EMBEDDING_DIM) + 'd.txt'\n",
    "#GLOVE_WORDS = 400000\n",
    "GLOVE_VOCSIZE = 400000\n",
    "INS_USERNAME_EMBEDDING_DIM = 20\n",
    "MAX_WORDS_IN_CAPTION = 40\n",
    "SPLIT_RATIO = [0.7, 0.15, 0.15] # training : validation : test\n",
    "\n",
    "data_path = os.path.join(BASE_DIR, DATA_DIR, DATA_FILE)\n",
    "data = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1337\n",
      "5\n",
      "6\n",
      "6\n",
      "58\n"
     ]
    }
   ],
   "source": [
    "N_ins_username = data.poster_instagram_username.nunique()\n",
    "N_poster_account_type = data.poster_account_type.nunique()\n",
    "N_media_type =data.media_type.nunique()\n",
    "N_caption_type = data.caption_type.nunique()\n",
    "\n",
    "raw_input_dim = MAX_WORDS_IN_CAPTION + 1 + \\\n",
    "                N_poster_account_type + N_media_type + N_caption_type\n",
    "\n",
    "print(N_ins_username)\n",
    "print(N_poster_account_type)\n",
    "print(N_media_type)\n",
    "print(N_caption_type)\n",
    "print(raw_input_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform the tabular data to usable numerial data to feed in the network\n",
    "## Transform caption and username data -- for embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the vocabulary is:  23376\n"
     ]
    }
   ],
   "source": [
    "# build word : vec dictionary from GloVe\n",
    "glove_path = os.path.join(BASE_DIR, GLOVE_DIR,  GLOVE_FILE)\n",
    "glove_index = {}\n",
    "with open(glove_path) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, dtype = 'float', sep=' ')\n",
    "        glove_index[word] = coefs\n",
    "\n",
    "# prepare tokenizer\n",
    "captions = data.media_caption.astype(str)\n",
    "token = Tokenizer()\n",
    "token.fit_on_texts(captions)\n",
    "vocab_size = len(token.word_index) + 1 # word_index is 1-indexed, puls 1-dim for OOV and padded words\n",
    "print(\"The size of the vocabulary is: \", vocab_size)\n",
    "\n",
    "# integer encode the documents\n",
    "encoded_captions = token.texts_to_sequences(captions)\n",
    "# pad documents to a max length of 4 words\n",
    "padded_captions = pad_sequences(encoded_captions, maxlen=MAX_WORDS_IN_CAPTION, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use \"represented_data\" matrix, to store the encoded data\n",
    "represented_data = padded_captions.copy()\n",
    "\n",
    "# create a weight matrix for all words in captions\n",
    "embedding_matrix = np.zeros((vocab_size, WORD_EMBEDDING_DIM))\n",
    "for word, idx in token.word_index.items(): # word_index is 1-indexed, zero vector for padding\n",
    "    embedding_vector = glove_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[idx] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# poster_instagram_username N_ins_username dim categorial, needs to be embedded to INS_USERNAME_EMBEDDING_DIM dim\n",
    "def OrdinalEncoder(categories):\n",
    "    \"\"\" An ordinal Encoder for data pre-processing,\n",
    "        Not strictly ordinal, but does the work.\n",
    "    \n",
    "        Args:\n",
    "            categories: numpy arrary or list of categories              \n",
    "        Returns:\n",
    "            numpy array of ordinal encoded entries        \n",
    "    \"\"\"\n",
    "    counter = Counter(categories).most_common() # indices assigned based on number of occurances\n",
    "    unique_ctgrs = [x[0] for x in counter] # list of categories, guaranteed in fixed order due to sorting\n",
    "    encoder = {ctgr : idx for idx, ctgr in enumerate(unique_ctgrs)}\n",
    "        \n",
    "    res = [0] * len(categories)\n",
    "    res = [encoder[ctgr] for ctgr in categories]\n",
    "    return np.array(res, dtype='int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ins_usernames = data.poster_instagram_username\n",
    "encoded_ins_usernames = OrdinalEncoder(ins_usernames)\n",
    "encoded_ins_usernames = np.array(encoded_ins_usernames).reshape(-1, 1)\n",
    "# concatenate encoded_ins_usernames to represented_data\n",
    "represented_data = np.concatenate((represented_data, encoded_ins_usernames), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101914\n",
      "41\n"
     ]
    }
   ],
   "source": [
    "print(len(represented_data))\n",
    "print(len(represented_data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform poster_account_type, media_type and caption_type -- one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ctgr in ['poster_account_type', 'media_type', 'caption_type']:\n",
    "    col=data[ctgr]\n",
    "    one_hot_encoded = to_categorical(OrdinalEncoder(col))\n",
    "    represented_data = np.concatenate((represented_data, one_hot_encoded), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101914\n",
      "58\n"
     ]
    }
   ],
   "source": [
    "print(len(represented_data))\n",
    "print(len(represented_data[0]))\n",
    "if len(represented_data[0]) != raw_input_dim:\n",
    "    print(\"Input dimension is wrong!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split all data into training, validation and test sets\n",
    "## Multi-hot encoding for multi-labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_hot_encoding(labels):\n",
    "    \"\"\" multi-hot encoding for the labels of each meida post \n",
    "        Args:\n",
    "            labels: list, dimension of rows may not be same\n",
    "        Return:\n",
    "            num_of_classes: int, total number of classes\n",
    "            res: array, num_of_examples x num_of_classes, multi-hot encoded array\n",
    "    \"\"\"\n",
    "    # number of classes\n",
    "    num_examples, min_idx, max_idx = len(labels), float('inf'), -float('inf')\n",
    "    for row in labels:\n",
    "        for col in row:\n",
    "            if col < min_idx:\n",
    "                min_idx = col\n",
    "            if col > max_idx:\n",
    "                max_idx = col\n",
    "    num_of_classes = max_idx - min_idx + 1\n",
    "    \n",
    "    res = np.zeros((num_examples, max_idx-min_idx+1), dtype = int)\n",
    "    for row in range(len(labels)):\n",
    "        lbls = labels[row]\n",
    "        for col in lbls:\n",
    "            res[row, col] = 1\n",
    "    return num_of_classes, res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_str = data.label.values\n",
    "labels = []\n",
    "for str_label in labels_str:\n",
    "    label = [int(i) for i in str_label[1:-1].split(',')]\n",
    "    labels.append(label)\n",
    "\n",
    "num_of_classes, encoded_labels_arr = multi_hot_encoding(labels)\n",
    "represented_data = np.concatenate((represented_data, encoded_labels_arr), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(orig_data, split_ratio, num_of_classes):\n",
    "    \"\"\" Split dataframe into train, validation and test datasets.\"\"\"\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(orig_data)\n",
    "    data_after_split, tmp, length = {}, {}, len(orig_data)\n",
    "    \n",
    "    tmp['train'], tmp['val'], tmp['test'] = np.split(\n",
    "        orig_data, [int(split_ratio[0] * length), int(1 - split_ratio[2] * length)], axis=0)\n",
    "    \n",
    "    for sett in ['train', 'val', 'test']:\n",
    "        X = tmp[sett][:, :-num_of_classes]\n",
    "        Y = tmp[sett][:, -num_of_classes:]\n",
    "        data_after_split['X' + '_' + sett] = X\n",
    "        data_after_split['Y' + '_' + sett] = Y\n",
    "\n",
    "    print(data_after_split['X_train'].shape, data_after_split['Y_train'].shape,\n",
    "          data_after_split['X_val'].shape, data_after_split['Y_val'].shape,\n",
    "          data_after_split['X_test'].shape, data_after_split['Y_test'].shape)\n",
    "    return data_after_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101914, 98)\n",
      "(71339, 58) (71339, 40) (15289, 58) (15289, 40) (15286, 58) (15286, 40)\n"
     ]
    }
   ],
   "source": [
    "print(represented_data.shape)\n",
    "data_after_split = split_data(represented_data, SPLIT_RATIO, num_of_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now the tabular data are ready to be fed into embedding layers and feed-forward network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/liangli/.local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-14-ffeb346838cf>:83: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 40)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "word_emb (Embedding)            (None, 40, 100)      2337600     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 100)          0           word_emb[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "insID_emb (Embedding)           (None, 1, 20)        26740       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 100)          0           lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 20)           0           insID_emb[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 17)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 137)          0           reshape_1[0][0]                  \n",
      "                                                                 reshape_2[0][0]                  \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 64)           8832        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 32)           2080        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 40)           1320        dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,376,572\n",
      "Trainable params: 2,376,572\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From /Users/liangli/.local/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 71339 samples, validate on 15289 samples\n",
      "Epoch 1/50\n",
      "71339/71339 [==============================] - 16s 218us/step - loss: 0.2067 - precision: 0.0031 - recall: 0.0169 - exact_match_ratio: 2.3830e-04 - val_loss: 0.1625 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_exact_match_ratio: 0.0000e+00\n",
      "Epoch 2/50\n",
      "71339/71339 [==============================] - 14s 201us/step - loss: 0.1540 - precision: 0.6624 - recall: 0.0139 - exact_match_ratio: 0.0199 - val_loss: 0.1441 - val_precision: 0.9812 - val_recall: 0.0217 - val_exact_match_ratio: 0.0309\n",
      "Epoch 3/50\n",
      "71339/71339 [==============================] - 15s 209us/step - loss: 0.1400 - precision: 0.9079 - recall: 0.0309 - exact_match_ratio: 0.0365 - val_loss: 0.1334 - val_precision: 0.7946 - val_recall: 0.0584 - val_exact_match_ratio: 0.0530\n",
      "Epoch 4/50\n",
      "71339/71339 [==============================] - 14s 199us/step - loss: 0.1310 - precision: 0.8733 - recall: 0.0768 - exact_match_ratio: 0.0850 - val_loss: 0.1277 - val_precision: 0.8692 - val_recall: 0.0966 - val_exact_match_ratio: 0.1049\n",
      "Epoch 5/50\n",
      "71339/71339 [==============================] - 14s 194us/step - loss: 0.1267 - precision: 0.8654 - recall: 0.1076 - exact_match_ratio: 0.1172 - val_loss: 0.1242 - val_precision: 0.8518 - val_recall: 0.1284 - val_exact_match_ratio: 0.1374\n",
      "Epoch 6/50\n",
      "71339/71339 [==============================] - 14s 191us/step - loss: 0.1238 - precision: 0.8683 - recall: 0.1233 - exact_match_ratio: 0.1335 - val_loss: 0.1220 - val_precision: 0.8523 - val_recall: 0.1414 - val_exact_match_ratio: 0.1502\n",
      "Epoch 7/50\n",
      "71339/71339 [==============================] - 14s 191us/step - loss: 0.1216 - precision: 0.8559 - recall: 0.1350 - exact_match_ratio: 0.1432 - val_loss: 0.1202 - val_precision: 0.8500 - val_recall: 0.1416 - val_exact_match_ratio: 0.1457\n",
      "Epoch 8/50\n",
      "71339/71339 [==============================] - 14s 201us/step - loss: 0.1195 - precision: 0.8497 - recall: 0.1468 - exact_match_ratio: 0.1505 - val_loss: 0.1180 - val_precision: 0.8382 - val_recall: 0.1606 - val_exact_match_ratio: 0.1589\n",
      "Epoch 9/50\n",
      "71339/71339 [==============================] - 15s 212us/step - loss: 0.1177 - precision: 0.8410 - recall: 0.1595 - exact_match_ratio: 0.1581 - val_loss: 0.1163 - val_precision: 0.8034 - val_recall: 0.1804 - val_exact_match_ratio: 0.1712\n",
      "Epoch 10/50\n",
      "71339/71339 [==============================] - 15s 206us/step - loss: 0.1160 - precision: 0.8300 - recall: 0.1680 - exact_match_ratio: 0.1639 - val_loss: 0.1152 - val_precision: 0.8199 - val_recall: 0.1812 - val_exact_match_ratio: 0.1727\n",
      "Epoch 11/50\n",
      "71339/71339 [==============================] - 15s 204us/step - loss: 0.1145 - precision: 0.8261 - recall: 0.1762 - exact_match_ratio: 0.1695 - val_loss: 0.1138 - val_precision: 0.8464 - val_recall: 0.1772 - val_exact_match_ratio: 0.1782\n",
      "Epoch 12/50\n",
      "71339/71339 [==============================] - 15s 207us/step - loss: 0.1132 - precision: 0.8275 - recall: 0.1855 - exact_match_ratio: 0.1775 - val_loss: 0.1126 - val_precision: 0.8154 - val_recall: 0.1945 - val_exact_match_ratio: 0.1878\n",
      "Epoch 13/50\n",
      "71339/71339 [==============================] - 15s 209us/step - loss: 0.1120 - precision: 0.8257 - recall: 0.1940 - exact_match_ratio: 0.1856 - val_loss: 0.1115 - val_precision: 0.8260 - val_recall: 0.2038 - val_exact_match_ratio: 0.1957\n",
      "Epoch 14/50\n",
      "71339/71339 [==============================] - 15s 211us/step - loss: 0.1109 - precision: 0.8268 - recall: 0.2007 - exact_match_ratio: 0.1916 - val_loss: 0.1106 - val_precision: 0.8143 - val_recall: 0.2106 - val_exact_match_ratio: 0.2023\n",
      "Epoch 15/50\n",
      "71339/71339 [==============================] - 14s 200us/step - loss: 0.1099 - precision: 0.8263 - recall: 0.2093 - exact_match_ratio: 0.1991 - val_loss: 0.1098 - val_precision: 0.8295 - val_recall: 0.2115 - val_exact_match_ratio: 0.2013\n",
      "Epoch 16/50\n",
      "71339/71339 [==============================] - 14s 200us/step - loss: 0.1090 - precision: 0.8282 - recall: 0.2141 - exact_match_ratio: 0.2035 - val_loss: 0.1090 - val_precision: 0.8126 - val_recall: 0.2240 - val_exact_match_ratio: 0.2129\n",
      "Epoch 17/50\n",
      "71339/71339 [==============================] - 14s 200us/step - loss: 0.1082 - precision: 0.8287 - recall: 0.2198 - exact_match_ratio: 0.2085 - val_loss: 0.1082 - val_precision: 0.8253 - val_recall: 0.2250 - val_exact_match_ratio: 0.2145\n",
      "Epoch 18/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71339/71339 [==============================] - 15s 203us/step - loss: 0.1074 - precision: 0.8283 - recall: 0.2244 - exact_match_ratio: 0.2112 - val_loss: 0.1079 - val_precision: 0.8420 - val_recall: 0.2214 - val_exact_match_ratio: 0.2189\n",
      "Epoch 19/50\n",
      "71339/71339 [==============================] - 14s 192us/step - loss: 0.1067 - precision: 0.8281 - recall: 0.2293 - exact_match_ratio: 0.2159 - val_loss: 0.1069 - val_precision: 0.8142 - val_recall: 0.2420 - val_exact_match_ratio: 0.2268\n",
      "Epoch 20/50\n",
      "71339/71339 [==============================] - 14s 196us/step - loss: 0.1061 - precision: 0.8287 - recall: 0.2344 - exact_match_ratio: 0.2199 - val_loss: 0.1065 - val_precision: 0.8276 - val_recall: 0.2310 - val_exact_match_ratio: 0.2221\n",
      "Epoch 21/50\n",
      "71339/71339 [==============================] - 14s 202us/step - loss: 0.1055 - precision: 0.8286 - recall: 0.2379 - exact_match_ratio: 0.2229 - val_loss: 0.1059 - val_precision: 0.8206 - val_recall: 0.2481 - val_exact_match_ratio: 0.2316\n",
      "Epoch 22/50\n",
      "71339/71339 [==============================] - 14s 202us/step - loss: 0.1050 - precision: 0.8297 - recall: 0.2427 - exact_match_ratio: 0.2271 - val_loss: 0.1054 - val_precision: 0.8228 - val_recall: 0.2527 - val_exact_match_ratio: 0.2409\n",
      "Epoch 23/50\n",
      "71339/71339 [==============================] - 14s 202us/step - loss: 0.1044 - precision: 0.8302 - recall: 0.2464 - exact_match_ratio: 0.2305 - val_loss: 0.1050 - val_precision: 0.8057 - val_recall: 0.2670 - val_exact_match_ratio: 0.2448\n",
      "Epoch 24/50\n",
      "71339/71339 [==============================] - 14s 203us/step - loss: 0.1039 - precision: 0.8290 - recall: 0.2515 - exact_match_ratio: 0.2347 - val_loss: 0.1046 - val_precision: 0.8276 - val_recall: 0.2537 - val_exact_match_ratio: 0.2427\n",
      "Epoch 25/50\n",
      "71339/71339 [==============================] - 15s 203us/step - loss: 0.1034 - precision: 0.8279 - recall: 0.2554 - exact_match_ratio: 0.2370 - val_loss: 0.1042 - val_precision: 0.8178 - val_recall: 0.2666 - val_exact_match_ratio: 0.2468\n",
      "Epoch 26/50\n",
      "71339/71339 [==============================] - 15s 206us/step - loss: 0.1029 - precision: 0.8288 - recall: 0.2598 - exact_match_ratio: 0.2411 - val_loss: 0.1036 - val_precision: 0.8179 - val_recall: 0.2723 - val_exact_match_ratio: 0.2517\n",
      "Epoch 27/50\n",
      "71339/71339 [==============================] - 15s 206us/step - loss: 0.1024 - precision: 0.8272 - recall: 0.2651 - exact_match_ratio: 0.2452 - val_loss: 0.1033 - val_precision: 0.8359 - val_recall: 0.2589 - val_exact_match_ratio: 0.2504\n",
      "Epoch 28/50\n",
      "71339/71339 [==============================] - 15s 206us/step - loss: 0.1020 - precision: 0.8270 - recall: 0.2676 - exact_match_ratio: 0.2482 - val_loss: 0.1029 - val_precision: 0.8235 - val_recall: 0.2741 - val_exact_match_ratio: 0.2545\n",
      "Epoch 29/50\n",
      "71339/71339 [==============================] - 15s 206us/step - loss: 0.1016 - precision: 0.8270 - recall: 0.2740 - exact_match_ratio: 0.2532 - val_loss: 0.1028 - val_precision: 0.8302 - val_recall: 0.2551 - val_exact_match_ratio: 0.2471\n",
      "Epoch 30/50\n",
      "71339/71339 [==============================] - 15s 207us/step - loss: 0.1012 - precision: 0.8292 - recall: 0.2791 - exact_match_ratio: 0.2599 - val_loss: 0.1021 - val_precision: 0.8213 - val_recall: 0.2794 - val_exact_match_ratio: 0.2601\n",
      "Epoch 31/50\n",
      "71339/71339 [==============================] - 15s 207us/step - loss: 0.1007 - precision: 0.8278 - recall: 0.2841 - exact_match_ratio: 0.2641 - val_loss: 0.1019 - val_precision: 0.8488 - val_recall: 0.2632 - val_exact_match_ratio: 0.2561\n",
      "Epoch 32/50\n",
      "71339/71339 [==============================] - 15s 207us/step - loss: 0.1004 - precision: 0.8301 - recall: 0.2878 - exact_match_ratio: 0.2684 - val_loss: 0.1015 - val_precision: 0.8267 - val_recall: 0.2938 - val_exact_match_ratio: 0.2784\n",
      "Epoch 33/50\n",
      "71339/71339 [==============================] - 15s 207us/step - loss: 0.1000 - precision: 0.8303 - recall: 0.2933 - exact_match_ratio: 0.2738 - val_loss: 0.1012 - val_precision: 0.7989 - val_recall: 0.3085 - val_exact_match_ratio: 0.2818\n",
      "Epoch 34/50\n",
      "71339/71339 [==============================] - 15s 206us/step - loss: 0.0997 - precision: 0.8292 - recall: 0.2978 - exact_match_ratio: 0.2787 - val_loss: 0.1010 - val_precision: 0.8170 - val_recall: 0.3064 - val_exact_match_ratio: 0.2849\n",
      "Epoch 35/50\n",
      "71339/71339 [==============================] - 15s 206us/step - loss: 0.0993 - precision: 0.8315 - recall: 0.3029 - exact_match_ratio: 0.2831 - val_loss: 0.1009 - val_precision: 0.8055 - val_recall: 0.3102 - val_exact_match_ratio: 0.2881\n",
      "Epoch 36/50\n",
      "71339/71339 [==============================] - 15s 205us/step - loss: 0.0989 - precision: 0.8306 - recall: 0.3064 - exact_match_ratio: 0.2864 - val_loss: 0.1004 - val_precision: 0.8248 - val_recall: 0.3059 - val_exact_match_ratio: 0.2935\n",
      "Epoch 37/50\n",
      "71339/71339 [==============================] - 15s 205us/step - loss: 0.0986 - precision: 0.8317 - recall: 0.3098 - exact_match_ratio: 0.2900 - val_loss: 0.1001 - val_precision: 0.7991 - val_recall: 0.3261 - val_exact_match_ratio: 0.2967\n",
      "Epoch 38/50\n",
      "71339/71339 [==============================] - 15s 210us/step - loss: 0.0983 - precision: 0.8306 - recall: 0.3128 - exact_match_ratio: 0.2932 - val_loss: 0.0998 - val_precision: 0.8193 - val_recall: 0.3108 - val_exact_match_ratio: 0.2953\n",
      "Epoch 39/50\n",
      "71339/71339 [==============================] - 15s 204us/step - loss: 0.0980 - precision: 0.8305 - recall: 0.3147 - exact_match_ratio: 0.2952 - val_loss: 0.0996 - val_precision: 0.8205 - val_recall: 0.3135 - val_exact_match_ratio: 0.2956\n",
      "Epoch 40/50\n",
      "71339/71339 [==============================] - 15s 204us/step - loss: 0.0977 - precision: 0.8328 - recall: 0.3191 - exact_match_ratio: 0.2991 - val_loss: 0.0993 - val_precision: 0.8312 - val_recall: 0.3018 - val_exact_match_ratio: 0.2865\n",
      "Epoch 41/50\n",
      "71339/71339 [==============================] - 15s 205us/step - loss: 0.0974 - precision: 0.8323 - recall: 0.3191 - exact_match_ratio: 0.3002 - val_loss: 0.0990 - val_precision: 0.8237 - val_recall: 0.3192 - val_exact_match_ratio: 0.3060\n",
      "Epoch 42/50\n",
      "71339/71339 [==============================] - 15s 204us/step - loss: 0.0971 - precision: 0.8322 - recall: 0.3237 - exact_match_ratio: 0.3041 - val_loss: 0.0988 - val_precision: 0.8253 - val_recall: 0.3146 - val_exact_match_ratio: 0.2989\n",
      "Epoch 43/50\n",
      "71339/71339 [==============================] - 15s 206us/step - loss: 0.0968 - precision: 0.8325 - recall: 0.3250 - exact_match_ratio: 0.3057 - val_loss: 0.0987 - val_precision: 0.7972 - val_recall: 0.3452 - val_exact_match_ratio: 0.3159\n",
      "Epoch 44/50\n",
      "71339/71339 [==============================] - 15s 205us/step - loss: 0.0966 - precision: 0.8330 - recall: 0.3283 - exact_match_ratio: 0.3084 - val_loss: 0.0985 - val_precision: 0.8254 - val_recall: 0.3169 - val_exact_match_ratio: 0.2988\n",
      "Epoch 45/50\n",
      "71339/71339 [==============================] - 14s 200us/step - loss: 0.0963 - precision: 0.8324 - recall: 0.3312 - exact_match_ratio: 0.3103 - val_loss: 0.0983 - val_precision: 0.8199 - val_recall: 0.3285 - val_exact_match_ratio: 0.3119\n",
      "Epoch 46/50\n",
      "71339/71339 [==============================] - 14s 203us/step - loss: 0.0960 - precision: 0.8350 - recall: 0.3334 - exact_match_ratio: 0.3134 - val_loss: 0.0980 - val_precision: 0.8345 - val_recall: 0.3150 - val_exact_match_ratio: 0.3062\n",
      "Epoch 47/50\n",
      "71339/71339 [==============================] - 14s 202us/step - loss: 0.0958 - precision: 0.8349 - recall: 0.3354 - exact_match_ratio: 0.3161 - val_loss: 0.0979 - val_precision: 0.8068 - val_recall: 0.3393 - val_exact_match_ratio: 0.3164\n",
      "Epoch 48/50\n",
      "71339/71339 [==============================] - 15s 203us/step - loss: 0.0955 - precision: 0.8348 - recall: 0.3380 - exact_match_ratio: 0.3183 - val_loss: 0.0975 - val_precision: 0.8195 - val_recall: 0.3348 - val_exact_match_ratio: 0.3172\n",
      "Epoch 49/50\n",
      "71339/71339 [==============================] - 14s 203us/step - loss: 0.0953 - precision: 0.8366 - recall: 0.3410 - exact_match_ratio: 0.3219 - val_loss: 0.0973 - val_precision: 0.8196 - val_recall: 0.3389 - val_exact_match_ratio: 0.3189\n",
      "Epoch 50/50\n",
      "71339/71339 [==============================] - 14s 203us/step - loss: 0.0951 - precision: 0.8353 - recall: 0.3437 - exact_match_ratio: 0.3250 - val_loss: 0.0974 - val_precision: 0.8158 - val_recall: 0.3400 - val_exact_match_ratio: 0.3206\n"
     ]
    }
   ],
   "source": [
    "# To ensure reproducible results\n",
    "# https://keras.io/getting-started/faq/#how-can-i-obtain-reproducible-results-using-keras-during-development\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1,\n",
    "                              inter_op_parallelism_threads=1)\n",
    "tf.set_random_seed(42)\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)\n",
    "\n",
    "# Integer IDs representing 1-hot encodings\n",
    "media_caption_in = Input(shape=(MAX_WORDS_IN_CAPTION,))\n",
    "poster_instagram_username_in = Input(shape=(1,))\n",
    "others_in = Input(shape=(N_poster_account_type + N_media_type + N_caption_type, ))\n",
    "\n",
    "# Two Embedding layers\n",
    "caption_embedding = Embedding(input_dim=vocab_size, output_dim=WORD_EMBEDDING_DIM, \n",
    "                              embeddings_initializer=Constant(embedding_matrix), \n",
    "                              input_length=MAX_WORDS_IN_CAPTION, trainable=True, \n",
    "                              name = 'word_emb')(media_caption_in)\n",
    "\n",
    "ins_username_embedding = Embedding(N_ins_username, INS_USERNAME_EMBEDDING_DIM, \n",
    "                                   name = 'insID_emb')(poster_instagram_username_in)\n",
    "\n",
    "# Reshape and merge all embeddings together\n",
    "avg_caption_embedding = Lambda(lambda x: K.mean(x, axis = 1, keepdims=False))(caption_embedding)\n",
    "\n",
    "reshape_caption_embedding = Reshape(target_shape=(WORD_EMBEDDING_DIM,))\n",
    "reshape_ins_username = Reshape(target_shape=(INS_USERNAME_EMBEDDING_DIM,))\n",
    "\n",
    "combined = concatenate([reshape_caption_embedding(avg_caption_embedding), \n",
    "                  reshape_ins_username(ins_username_embedding), others_in])\n",
    "    \n",
    "\n",
    "\n",
    "# Hidden layers\n",
    "regularizer_param = 0.0003\n",
    "hidden_1 = Dense(64, activation='relu', \n",
    "                 kernel_regularizer=regularizers.l2(regularizer_param))(combined)\n",
    "hidden_2 = Dense(32, activation='relu', \n",
    "                 kernel_regularizer=regularizers.l2(regularizer_param))(hidden_1)\n",
    "output = Dense(num_of_classes, activation='sigmoid', \n",
    "               kernel_regularizer=regularizers.l2(regularizer_param))(hidden_2)\n",
    "\n",
    "# Compile with categorical crossentropy and adam\n",
    "model = Model(inputs=[media_caption_in, poster_instagram_username_in, others_in], outputs = [output])\n",
    "\n",
    "##----- Important: In multi-label classification problems. It is -----##\n",
    "##----- easy to obtain high accuracy because of sparsity of labels -----##\n",
    "##----- Use customized metrics: precision, recall, and Exact Match Ratio(Subset accuracy) -----##\n",
    "## https://towardsdatascience.com/journey-to-the-center-of-multi-label-classification-384c40229bff\n",
    "#https://stats.stackexchange.com/questions/12702/what-are-the-measure-for-accuracy-of-multilabel-data/168952\n",
    "\n",
    "#There is a difference between the metric on training dataset and on validation dataset. \n",
    "#For the val set the metric is calculated at epoch end for your whole val dataset. \n",
    "#For the train set: The metric is calculated on batch end and the average keeps getting updated till epochs end.\n",
    "#As you can see the metric for the train set is evaluated on the fly with each batch was evaluated using different weights. \n",
    "#That's why the train metric shows sometimes strange behaviour.\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    #Precision metric.\n",
    "    y_pred = K.cast(K.greater(y_pred,0.5),dtype=float)\n",
    "    true_positives = K.sum(K.round(y_true * y_pred))\n",
    "    predicted_positives = K.sum(K.round(y_pred))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    #Recall metric.\n",
    "    y_pred = K.cast(K.greater(y_pred,0.5),dtype=float)\n",
    "    true_positives = K.sum(K.round(y_true * y_pred))\n",
    "    actural_positives = K.sum(K.round(y_true))\n",
    "    recall = true_positives / (actural_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "#def Hamming_loss(y_true, y_pred):\n",
    "#    # y_pred is not filterd by 0.5 yet\n",
    "#    tmp = K.abs(y_true-y_pred)\n",
    "#    return K.mean(K.cast(K.greater(tmp,0.5),dtype=float))\n",
    "\n",
    "def exact_match_ratio(y_true, y_pred):\n",
    "    #pred = tf.equal(tf.round(y_logits), tf.round(y_true))\n",
    "    predictions = tf.to_float(tf.greater_equal(y_pred, 0.5))\n",
    "    pred_match = tf.equal(predictions, tf.round(y_true))\n",
    "    exact_match = tf.reduce_min(tf.to_float(pred_match), axis=1)\n",
    "    return tf.reduce_mean(exact_match)    \n",
    "    \n",
    "model.compile(loss='binary_crossentropy', \n",
    "            optimizer='adam', metrics=[precision, recall, exact_match_ratio])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "val_data = ([data_after_split['X_val'][:, :MAX_WORDS_IN_CAPTION], \n",
    "             data_after_split['X_val'][:,MAX_WORDS_IN_CAPTION], \n",
    "             data_after_split['X_val'][:,MAX_WORDS_IN_CAPTION+1:]],\n",
    "             data_after_split['Y_val'])\n",
    "batchSize = 128\n",
    "\n",
    "history = model.fit([data_after_split['X_train'][:, :MAX_WORDS_IN_CAPTION], \n",
    "                     data_after_split['X_train'][:,MAX_WORDS_IN_CAPTION], \n",
    "                     data_after_split['X_train'][:,MAX_WORDS_IN_CAPTION+1:]],\n",
    "                     data_after_split['Y_train'],\n",
    "                     validation_data = val_data, batch_size=batchSize, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model and history\n",
    "with open('DNN_history', 'wb') as fin:\n",
    "        pickle.dump(history.history, fin)\n",
    "model.save('DNN_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a35e2c3c8>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_precision(y_true, y_pred):\n",
    "    #Precision metric.\n",
    "    y_pred[y_pred >= 0.5] = 1\n",
    "    y_pred[y_pred <0.5] = 0\n",
    "    true_positives = np.sum(y_pred*y_true)\n",
    "    predicted_positives = np.sum(y_pred)\n",
    "    return true_positives / predicted_positives\n",
    "\n",
    "def get_recall(y_true, y_pred):\n",
    "    #Recall metric.\n",
    "    y_pred[y_pred >= 0.5] = 1\n",
    "    y_pred[y_pred <0.5] = 0\n",
    "    true_positives = np.sum(y_pred*y_true)\n",
    "    actural_positives = np.sum(y_true)\n",
    "    return true_positives / actural_positives\n",
    "\n",
    "def get_exact_match_ratio(y_true, y_pred):\n",
    "    match = np.equal(y_pred, y_true)\n",
    "    match = np.amin(match, axis=1)\n",
    "    exact_match_ratio = np.mean(match)\n",
    "    return exact_match_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision, recall and exact match ratio for test set are 0.7975, 0.3263 and 0.3057\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict([data_after_split['X_test'][:, :MAX_WORDS_IN_CAPTION], \n",
    "               data_after_split['X_test'][:,MAX_WORDS_IN_CAPTION], \n",
    "               data_after_split['X_test'][:,MAX_WORDS_IN_CAPTION+1:]])\n",
    "y_true = data_after_split['Y_test']\n",
    "\n",
    "precision, recall,  = get_precision(y_true, y_pred), get_recall(y_true, y_pred)\n",
    "exact_match_ratio = get_exact_match_ratio(y_true, y_pred)\n",
    "\n",
    "print(\"precision, recall and exact match ratio for test set are %.4f, %.4f and %.4f\" \\\n",
    "      %(precision, recall, exact_match_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "         eyebrow       0.58      0.19      0.29       326\n",
      "     moisturizer       0.40      0.00      0.01       405\n",
      "       eyeshadow       0.00      0.00      0.00       311\n",
      "         mascara       0.45      0.15      0.22       613\n",
      "           brush       0.88      0.44      0.59       498\n",
      "       fragrance       0.90      0.53      0.66       480\n",
      "        cleanser       0.78      0.10      0.18       662\n",
      "            tool       0.87      0.66      0.75       953\n",
      "     nail polish       0.98      0.78      0.87       506\n",
      "     eye palette       0.70      0.30      0.42       880\n",
      "            wash       0.73      0.68      0.70       534\n",
      "styling products       0.75      0.70      0.72       771\n",
      "      treatments       0.73      0.05      0.09       844\n",
      "          powder       0.00      0.00      0.00       438\n",
      "            mask       0.00      0.00      0.00       292\n",
      "     body lotion       0.70      0.34      0.46       565\n",
      "     conditioner       0.78      0.68      0.72       632\n",
      "   cheek palette       0.00      0.00      0.00       650\n",
      "        lipstick       0.61      0.27      0.37      1003\n",
      " hair treatments       0.84      0.38      0.52       516\n",
      "     highlighter       1.00      0.00      0.00       414\n",
      "         shampoo       0.81      0.71      0.76       777\n",
      "   setting spray       1.00      0.03      0.05       556\n",
      "           blush       0.00      0.00      0.00       336\n",
      "    face suncare       0.86      0.47      0.61       501\n",
      "        eyeliner       0.31      0.01      0.03       291\n",
      "    face palette       0.73      0.18      0.29       655\n",
      "      foundation       0.52      0.13      0.21       475\n",
      "        lip balm       0.89      0.54      0.67       625\n",
      "       face mist       0.00      0.00      0.00       441\n",
      "         eyecare       0.81      0.28      0.42       517\n",
      "            lash       0.97      0.78      0.87       798\n",
      "       lip gloss       0.00      0.00      0.00       310\n",
      "     face primer       0.00      0.00      0.00       469\n",
      "       concealer       0.49      0.04      0.07       569\n",
      "         bronzer       0.56      0.08      0.13       638\n",
      "           toner       0.65      0.04      0.07       403\n",
      "       lip liner       0.68      0.40      0.50       520\n",
      "    body suncare       0.87      0.82      0.84       561\n",
      "    body glitter       0.91      0.77      0.84       536\n",
      "\n",
      "       micro avg       0.80      0.33      0.46     22271\n",
      "       macro avg       0.59      0.29      0.35     22271\n",
      "    weighted avg       0.64      0.33      0.39     22271\n",
      "     samples avg       0.39      0.37      0.37     22271\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "classID_className = {0: 'color cosmetics:eye:eyebrow', 1: 'skincare:skincare:moisturizer', 2: 'color cosmetics:eye:eyeshadow', 3: 'color cosmetics:eye:mascara', 4: 'accessories:accessories:brush', 5: 'fragrance:fragrance:fragrance', 6: 'skincare:skincare:cleanser', 7: 'accessories:accessories:tool', 8: 'nail:nail:nail polish', 9: 'color cosmetics:eye:eye palette', 10: 'bath body:bath body:wash', 11: 'hair:style:styling products', 12: 'skincare:skincare:treatments', 13: 'color cosmetics:face:powder', 14: 'skincare:skincare:mask', 15: 'bath body:bath body:body lotion', 16: 'hair:cleanse:conditioner', 17: 'color cosmetics:cheek:cheek palette', 18: 'color cosmetics:lip:lipstick', 19: 'hair:treat:hair treatments', 20: 'color cosmetics:cheek:highlighter', 21: 'hair:cleanse:shampoo', 22: 'color cosmetics:face:setting spray', 23: 'color cosmetics:cheek:blush', 24: 'skincare:skincare:face suncare', 25: 'color cosmetics:eye:eyeliner', 26: 'color cosmetics:face:face palette', 27: 'color cosmetics:face:foundation', 28: 'color cosmetics:lip:lip balm', 29: 'skincare:skincare:face mist', 30: 'skincare:skincare:eyecare', 31: 'color cosmetics:eye:lash', 32: 'color cosmetics:lip:lip gloss', 33: 'color cosmetics:face:face primer', 34: 'color cosmetics:face:concealer', 35: 'color cosmetics:cheek:bronzer', 36: 'skincare:skincare:toner', 37: 'color cosmetics:lip:lip liner', 38: 'bath body:bath body:body suncare', 39: 'bath body:bath body:body glitter'}\n",
    "target_names = [i.split(':')[2] for i in classID_className.values()]\n",
    "print(classification_report(y_true, y_pred, target_names = target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
